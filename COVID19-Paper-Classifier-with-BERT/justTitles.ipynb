{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEA-F_FFzj9F",
        "outputId": "cee5e627-d514-4157-af1e-4dfed6fd8ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KvotKPHPgyS0"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "import nltk\n",
        "import sklearn\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYInlcQYg8ay",
        "outputId": "a91bb76d-453f-4d80-c664-9856bfc81d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of            pmid  ...                 label\n",
            "0      32519164  ...   Treatment;Mechanism\n",
            "1      32691006  ...  Treatment;Prevention\n",
            "2      32858315  ...           Case Report\n",
            "3      32985329  ...            Prevention\n",
            "4      32812051  ...             Treatment\n",
            "...         ...  ...                   ...\n",
            "24955  32508388  ...            Prevention\n",
            "24956  32815519  ...             Diagnosis\n",
            "24957  32904987  ...   Treatment;Diagnosis\n",
            "24958  32892181  ...            Prevention\n",
            "24959  32389144  ...            Prevention\n",
            "\n",
            "[24960 rows x 9 columns]>\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/BC7-LitCovid-Train.csv\")\n",
        "df2 = pd.read_csv(\"/content/BC7-LitCovid-Dev (1).csv\")\n",
        "\n",
        "print(df.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjQBvku-tZzx",
        "outputId": "780e45b9-929c-45c9-b65e-701eecd59fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rYX4sRGXxdr1"
      },
      "outputs": [],
      "source": [
        "testResults = pd.DataFrame(columns=['Treatment','Diagnosis','Prevention','Mechanism','Transmission','Epidemic Forecasting','Case Report'])\n",
        "\n",
        "for el in df2[\"label\"]:\n",
        "  rowList = []\n",
        "  if \"Treatment\" in el:\n",
        "    rowList.append(1)\n",
        "  else:\n",
        "    rowList.append(0)\n",
        "  if 'Diagnosis' in el:\n",
        "    rowList.append(1)\n",
        "  else:\n",
        "    rowList.append(0)\n",
        "  if 'Prevention' in el:\n",
        "    rowList.append(1)\n",
        "  else:\n",
        "    rowList.append(0)\n",
        "  if 'Mechanism' in el:\n",
        "    rowList.append(1)\n",
        "  else:\n",
        "    rowList.append(0)\n",
        "  if 'Transmission' in el:\n",
        "    rowList.append(1)\n",
        "  else:\n",
        "    rowList.append(0)\n",
        "  if 'Epidemic Forecasting' in el:\n",
        "    rowList.append(1)\n",
        "  else:\n",
        "    rowList.append(0)\n",
        "  if 'Case Report' in el:\n",
        "    rowList.append(1)\n",
        "  else:\n",
        "    rowList.append(0)\n",
        "\n",
        "  df_length = len(testResults)\n",
        "  testResults.loc[df_length] = rowList\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GneNyDOZ0haN",
        "outputId": "48f161fe-3d60-4479-87b5-1606cb9f4f78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gH0wRx24CdfY"
      },
      "outputs": [],
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "stopwords = stopwords.words('english')\n",
        "stopwords = set(stopwords)\n",
        "\n",
        "def tokenize_lemma_stopwords(text):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    # split string into words (tokens)\n",
        "    tokens = nltk.tokenize.word_tokenize(text.lower())\n",
        "    # keep strings with only alphabets\n",
        "    tokens = [t for t in tokens if t.isalpha()]\n",
        "    # put words into base form\n",
        "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] \n",
        "    tokens = [stemmer.stem(t) for t in tokens]\n",
        "    # remove short words, they're probably not useful\n",
        "    tokens = [t for t in tokens if len(t) > 2]\n",
        "    tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
        "    cleanedText = \" \".join(tokens)\n",
        "    return cleanedText\n",
        "\n",
        "def dataCleaning(df):\n",
        "    data = df.copy()\n",
        "    data[\"content\"] =(df[\"title\"]).apply(tokenize_lemma_stopwords)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A5zhXeZCfV3",
        "outputId": "e20bee65-da43-43b9-9daa-51e0012c5430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "counterAlpha = 0\n",
        "counterStopWords =0\n",
        "cleanedTrainData = dataCleaning(df)\n",
        "counterAlpha = 0\n",
        "counterStopWords =0\n",
        "cleanedTestData = dataCleaning(df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Y3hAWJHCLrKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec12caf-2f14-4b25-c768-e7fc3729329f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 1 0 1]\n",
            " [1 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 1 0 ... 0 0 1]\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 1 0 0]]\n",
            "[[0 0 0 ... 0 0 1]\n",
            " [0 0 1 ... 1 0 0]\n",
            " [0 0 0 ... 1 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 1 0 ... 0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "for i in range(len(cleanedTrainData[\"label\"])):\n",
        "  cleanedTrainData[\"label\"][i] = str(cleanedTrainData[\"label\"][i]).split(\";\")\n",
        "for i in range(len(cleanedTestData[\"label\"])):\n",
        "  cleanedTestData[\"label\"][i] = str(cleanedTestData[\"label\"][i]).split(\";\")\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "train_labels = mlb.fit_transform(cleanedTrainData[\"label\"])\n",
        "test_labels = mlb.transform(cleanedTestData[\"label\"])\n",
        "print(train_labels)\n",
        "print(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "v9_0LHyMarGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "del model\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjEZTkRfbReX",
        "outputId": "231c8472-4d87-42df-d78a-aac9d383d584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "629"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plrYcfswq8rN",
        "outputId": "45baf72e-99e5-48e4-a17b-d0b1f8f0810c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jan 14 12:59:53 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0    31W /  70W |  13184MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import sys"
      ],
      "metadata": {
        "id": "AkWNr-jyrKPV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "MAX_LEN = 20\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 1e-05"
      ],
      "metadata": {
        "id": "ejXTIzREsXVB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ],
      "metadata": {
        "id": "OpOVGuYJsXdA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "0Zr43T9QsXjL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, labels, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['content']\n",
        "        self.targets = labels\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.title[index])\n",
        "        title = \" \".join(title.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }"
      ],
      "metadata": {
        "id": "uBgGLDLks5W2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = cleanedTrainData[:20000]\n",
        "val_df = cleanedTrainData[20000:]\n",
        "test_df = cleanedTestData\n",
        "val_df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "UX6mTyhktH6-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_df,train_labels[:20000], tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df,train_labels[20000:], tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df,test_labels, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "B10TMD4xtq6A"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    drop_last=True\n",
        ")\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, \n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    drop_last=True\n",
        ")"
      ],
      "metadata": {
        "id": "kv8_5uQrtIDk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
      ],
      "metadata": {
        "id": "ZeHZxa6ZuWqe"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, 7)\n",
        "    \n",
        "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
        "        output = self.bert_model(\n",
        "            input_ids, \n",
        "            attention_mask=attn_mask, \n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        output = self.linear(output_dropout)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "veI6_fuwughI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "Q2vJ_lKfupGm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_targets=[]\n",
        "val_outputs=[]"
      ],
      "metadata": {
        "id": "yiY5P7JuuqGV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(n_epochs, training_loader, validation_loader, model, \n",
        "                optimizer, checkpoint_path, best_model_path):\n",
        "   \n",
        "  # initialize tracker for minimum validation loss\n",
        "  valid_loss_min = np.Inf\n",
        "   \n",
        " \n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
        "    for batch_idx, data in enumerate(training_loader):\n",
        "        #print('yyy epoch', batch_idx)\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        if batch_idx%50==0:\n",
        "            print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        #print('after loss data in training', loss.item(), train_loss)\n",
        "    \n",
        "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "    \n",
        "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        " \n",
        "    model.eval()\n",
        "   \n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(validation_loader, 0):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      #print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics \n",
        "      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "      \n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "        \n",
        "        # save checkpoint\n",
        "      #save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "        \n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "        #save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "KZa_hLb2uyt9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = \"/content/drive/MyDrive/datasets/multi-label/curr_ckpt\"\n",
        "best_model_path = \"/content/drive/MyDrive/datasets/multi-label/best_model.pt\""
      ],
      "metadata": {
        "id": "vpjN4s_Du1_K"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-x6XypEu4r5",
        "outputId": "09b483e1-1626-472f-f5d4-63e942a61f6b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############# Epoch 1: Training Start   #############\n",
            "Epoch: 1, Training Loss:  0.7173444628715515\n",
            "Epoch: 1, Training Loss:  0.48879945278167725\n",
            "Epoch: 1, Training Loss:  0.4460816979408264\n",
            "Epoch: 1, Training Loss:  0.3951476216316223\n",
            "Epoch: 1, Training Loss:  0.36039286851882935\n",
            "Epoch: 1, Training Loss:  0.3265092968940735\n",
            "Epoch: 1, Training Loss:  0.2973400056362152\n",
            "Epoch: 1, Training Loss:  0.29337021708488464\n",
            "Epoch: 1, Training Loss:  0.2888309955596924\n",
            "Epoch: 1, Training Loss:  0.3922474980354309\n",
            "Epoch: 1, Training Loss:  0.23836950957775116\n",
            "Epoch: 1, Training Loss:  0.19783827662467957\n",
            "Epoch: 1, Training Loss:  0.20539015531539917\n",
            "Epoch: 1, Training Loss:  0.35085830092430115\n",
            "Epoch: 1, Training Loss:  0.40098270773887634\n",
            "Epoch: 1, Training Loss:  0.4686267375946045\n",
            "Epoch: 1, Training Loss:  0.285885751247406\n",
            "Epoch: 1, Training Loss:  0.23905763030052185\n",
            "Epoch: 1, Training Loss:  0.2535453736782074\n",
            "Epoch: 1, Training Loss:  0.2787538766860962\n",
            "Epoch: 1, Training Loss:  0.3232952654361725\n",
            "Epoch: 1, Training Loss:  0.306117445230484\n",
            "Epoch: 1, Training Loss:  0.21183906495571136\n",
            "Epoch: 1, Training Loss:  0.24813219904899597\n",
            "Epoch: 1, Training Loss:  0.28219446539878845\n",
            "Epoch: 1, Training Loss:  0.11394549906253815\n",
            "Epoch: 1, Training Loss:  0.1877709925174713\n",
            "Epoch: 1, Training Loss:  0.4352242946624756\n",
            "Epoch: 1, Training Loss:  0.3850378394126892\n",
            "Epoch: 1, Training Loss:  0.18379157781600952\n",
            "Epoch: 1, Training Loss:  0.28456932306289673\n",
            "Epoch: 1, Training Loss:  0.3474874198436737\n",
            "Epoch: 1, Training Loss:  0.14169567823410034\n",
            "Epoch: 1, Training Loss:  0.3473372161388397\n",
            "Epoch: 1, Training Loss:  0.37235647439956665\n",
            "Epoch: 1, Training Loss:  0.2347474992275238\n",
            "Epoch: 1, Training Loss:  0.09835788607597351\n",
            "Epoch: 1, Training Loss:  0.2577887773513794\n",
            "Epoch: 1, Training Loss:  0.10824049264192581\n",
            "Epoch: 1, Training Loss:  0.20430117845535278\n",
            "Epoch: 1, Training Loss:  0.2896634340286255\n",
            "Epoch: 1, Training Loss:  0.22257861495018005\n",
            "Epoch: 1, Training Loss:  0.10881111025810242\n",
            "Epoch: 1, Training Loss:  0.2438570261001587\n",
            "Epoch: 1, Training Loss:  0.21133701503276825\n",
            "Epoch: 1, Training Loss:  0.1900016963481903\n",
            "Epoch: 1, Training Loss:  0.19529448449611664\n",
            "Epoch: 1, Training Loss:  0.23055580258369446\n",
            "Epoch: 1, Training Loss:  0.29795193672180176\n",
            "Epoch: 1, Training Loss:  0.1564597189426422\n",
            "############# Epoch 1: Training End     #############\n",
            "############# Epoch 1: Validation Start   #############\n",
            "############# Epoch 1: Validation End     #############\n",
            "Epoch: 1 \tAvgerage Training Loss: 0.000111 \tAverage Validation Loss: 0.000171\n",
            "Validation loss decreased (inf --> 0.000171).  Saving model ...\n",
            "############# Epoch 1  Done   #############\n",
            "\n",
            "############# Epoch 2: Training Start   #############\n",
            "Epoch: 2, Training Loss:  0.050649311393499374\n",
            "Epoch: 2, Training Loss:  0.16732022166252136\n",
            "Epoch: 2, Training Loss:  0.25898727774620056\n",
            "Epoch: 2, Training Loss:  0.1683713048696518\n",
            "Epoch: 2, Training Loss:  0.08773580193519592\n",
            "Epoch: 2, Training Loss:  0.19830621778964996\n",
            "Epoch: 2, Training Loss:  0.0994732454419136\n",
            "Epoch: 2, Training Loss:  0.07000617682933807\n",
            "Epoch: 2, Training Loss:  0.19100990891456604\n",
            "Epoch: 2, Training Loss:  0.1984749734401703\n",
            "Epoch: 2, Training Loss:  0.22345812618732452\n",
            "Epoch: 2, Training Loss:  0.13260015845298767\n",
            "Epoch: 2, Training Loss:  0.07640448957681656\n",
            "Epoch: 2, Training Loss:  0.3723539113998413\n",
            "Epoch: 2, Training Loss:  0.23624199628829956\n",
            "Epoch: 2, Training Loss:  0.19257532060146332\n",
            "Epoch: 2, Training Loss:  0.37382808327674866\n",
            "Epoch: 2, Training Loss:  0.231695294380188\n",
            "Epoch: 2, Training Loss:  0.14650386571884155\n",
            "Epoch: 2, Training Loss:  0.1972251683473587\n",
            "Epoch: 2, Training Loss:  0.05362343043088913\n",
            "Epoch: 2, Training Loss:  0.1844911426305771\n",
            "Epoch: 2, Training Loss:  0.2495356947183609\n",
            "Epoch: 2, Training Loss:  0.2001078575849533\n",
            "Epoch: 2, Training Loss:  0.09430842101573944\n",
            "Epoch: 2, Training Loss:  0.21569542586803436\n",
            "Epoch: 2, Training Loss:  0.3048523962497711\n",
            "Epoch: 2, Training Loss:  0.2769352197647095\n",
            "Epoch: 2, Training Loss:  0.15701444447040558\n",
            "Epoch: 2, Training Loss:  0.28173717856407166\n",
            "Epoch: 2, Training Loss:  0.15187303721904755\n",
            "Epoch: 2, Training Loss:  0.19245125353336334\n",
            "Epoch: 2, Training Loss:  0.12419424951076508\n",
            "Epoch: 2, Training Loss:  0.08264627307653427\n",
            "Epoch: 2, Training Loss:  0.3403832018375397\n",
            "Epoch: 2, Training Loss:  0.22087547183036804\n",
            "Epoch: 2, Training Loss:  0.09691780060529709\n",
            "Epoch: 2, Training Loss:  0.21659643948078156\n",
            "Epoch: 2, Training Loss:  0.17539151012897491\n",
            "Epoch: 2, Training Loss:  0.2593741714954376\n",
            "Epoch: 2, Training Loss:  0.09936083108186722\n",
            "Epoch: 2, Training Loss:  0.0801638588309288\n",
            "Epoch: 2, Training Loss:  0.1629200577735901\n",
            "Epoch: 2, Training Loss:  0.18578343093395233\n",
            "Epoch: 2, Training Loss:  0.213339164853096\n",
            "Epoch: 2, Training Loss:  0.2347617745399475\n",
            "Epoch: 2, Training Loss:  0.10332999378442764\n",
            "Epoch: 2, Training Loss:  0.26710134744644165\n",
            "Epoch: 2, Training Loss:  0.15667100250720978\n",
            "Epoch: 2, Training Loss:  0.13543011248111725\n",
            "############# Epoch 2: Training End     #############\n",
            "############# Epoch 2: Validation Start   #############\n",
            "############# Epoch 2: Validation End     #############\n",
            "Epoch: 2 \tAvgerage Training Loss: 0.000078 \tAverage Validation Loss: 0.000155\n",
            "Validation loss decreased (0.000171 --> 0.000155).  Saving model ...\n",
            "############# Epoch 2  Done   #############\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(test_data_loader, 0):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets"
      ],
      "metadata": {
        "id": "dsXkdLgG3ryA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "XENwJ8kjFOfS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, targets = test()\n",
        "outputs = np.array(outputs) >= 0.5\n",
        "accuracy = metrics.accuracy_score(targets, outputs)\n",
        "f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
        "f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
        "print(f\"Accuracy Score = {accuracy}\")\n",
        "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "print(f\"F1 Score (Macro) = {f1_score_macro}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CLcu3nr36V9",
        "outputId": "26e42ce8-1bae-494d-e23e-bba107a117b3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score = 0.6798780487804879\n",
            "F1 Score (Micro) = 0.8077698369067253\n",
            "F1 Score (Macro) = 0.7341940425540406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "h_t6cPeRL-vL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb50b175-b7be-4ea8-84fe-ee22d28c7f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "#[6:Treatment, 4:Prevention, 3:Mechanism, 1:Diagnosis , 2:Epidemic Forecasting , 5:Transmission , 0:Case Report  ]\n",
        "predictions = pd.DataFrame(np.nan, index=[i for i in range(6232)],columns=[\"Case Report\",\"Diagnosis\",\"Epidemic Forecasting\",\"Mechanism\",\"Prevention\",\"Transmission\",\"Treatment\"])\n",
        "#predictions = predictions.append(pd.DataFrame(knnPredictions, columns=predictions.columns))\n",
        "columns=[\"Case Report\",\"Diagnosis\",\"Epidemic Forecasting\",\"Mechanism\",\"Prevention\",\"Transmission\",\"Treatment\"]\n",
        "for i in range(7):\n",
        "  for j in range(6232):\n",
        "    predictions[columns[i]][j] = outputs[j][i]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.replace({False: 0, True: 1}, inplace=True)"
      ],
      "metadata": {
        "id": "wRWmyvb7H4re"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "W-Z6snHKOEWB"
      },
      "outputs": [],
      "source": [
        "predictions = predictions[[\"Treatment\",\"Diagnosis\", \"Prevention\", \"Mechanism\", \"Transmission\",\"Epidemic Forecasting\", \"Case Report\"]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "thki_t7EIKG5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "iBG2fHMaNevr"
      },
      "outputs": [],
      "source": [
        "predictions.to_csv(\"/content/predictions.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qKP8Nk2vNy3L"
      },
      "outputs": [],
      "source": [
        "testResults.to_csv(\"/content/testResults.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cT74ny7OS3f",
        "outputId": "2cda5bb5-60b4-4422-9781-34ecb4d12247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'biocreative_litcovid'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 32 (delta 15), reused 12 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (32/32), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ncbi/biocreative_litcovid.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq7TamV4Ok2n",
        "outputId": "675009e5-5702-4439-b66a-a01369199046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation starts...\n",
            "validation passes...\n",
            "label-based measures...\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           Treatment     0.8294    0.7985    0.8137      2204\n",
            "           Diagnosis     0.8178    0.7178    0.7646      1545\n",
            "          Prevention     0.8873    0.8857    0.8865      2747\n",
            "           Mechanism     0.7961    0.7292    0.7612      1071\n",
            "        Transmission     0.7818    0.3373    0.4712       255\n",
            "Epidemic Forecasting     0.8389    0.6510    0.7331       192\n",
            "         Case Report     0.7663    0.6598    0.7090       482\n",
            "\n",
            "           micro avg     0.8396    0.7782    0.8078      8496\n",
            "           macro avg     0.8168    0.6828    0.7342      8496\n",
            "        weighted avg     0.8370    0.7782    0.8036      8496\n",
            "         samples avg     0.8329    0.8129    0.8075      8496\n",
            "\n",
            "instance-based measures\n",
            "mean precision 0.8329\n",
            "mean recall 0.8129\n",
            "f1 0.8228\n"
          ]
        }
      ],
      "source": [
        "!python /content/biocreative_litcovid/biocreative_litcovid_eval.py --gold /content/testResults.csv --pred /content/predictions.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/model2.pt')"
      ],
      "metadata": {
        "id": "L4jHbMtcMDQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERTClass()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "Nvpcj6y0PMkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/model2.pt\"))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "zAOUCzD4Qhoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62pADqAZSTPF"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/predictions.csv') "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled6 (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}